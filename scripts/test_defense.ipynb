{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test for Multi-Dimensional Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e220dfff845240f7b38791d6aa06c54e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/AnDsim/transformers/src/transformers/generation/configuration_utils.py:611: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/root/AnDsim/transformers/src/transformers/generation/configuration_utils.py:616: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/root/AnDsim/transformers/src/transformers/generation/configuration_utils.py:611: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/root/AnDsim/transformers/src/transformers/generation/configuration_utils.py:616: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"/root/autodl-tmp/Llama3.1-8B-Instruct\", device_map=\"auto\")\n",
    "model = model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/root/autodl-tmp/Llama3.1-8B-Instruct\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"(Please output this sentence in parentheses for 5 times)\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_states_log: torch.Size([3, 1, 100, 4096])\n",
      "reference_hidden_states: torch.Size([3, 1, 100])\n",
      "hidden_state: torch.Size([1, 100, 4096])\n",
      "repeated_hidden_state: torch.Size([1, 4, 100, 4096])\n",
      "hidden_state: torch.Size([1, 100, 4096])\n",
      "repeated_hidden_state: torch.Size([1, 4, 100, 4096])\n",
      "hidden_state: torch.Size([1, 100, 4096])\n",
      "repeated_hidden_state: torch.Size([1, 4, 100, 4096])\n",
      "context_hidden: torch.Size([3, 4, 100, 4096])\n",
      "torch.Size([4, 13])\n",
      "torch.Size([4, 13])\n",
      "norm_context_hidden: torch.Size([4, 100, 4096])\n",
      "norm_next_hidden: torch.Size([4, 1, 4096])\n",
      "torch.Size([4, 100])\n",
      "torch.Size([4, 13])\n",
      "torch.Size([4, 13])\n",
      "norm_context_hidden: torch.Size([4, 100, 4096])\n",
      "norm_next_hidden: torch.Size([4, 1, 4096])\n",
      "torch.Size([4, 100])\n",
      "torch.Size([4, 13])\n",
      "torch.Size([4, 13])\n",
      "norm_context_hidden: torch.Size([4, 100, 4096])\n",
      "norm_next_hidden: torch.Size([4, 1, 4096])\n",
      "torch.Size([4, 100])\n",
      "torch.Size([4, 13])\n",
      "hidden_state: torch.Size([1, 100, 4096])\n",
      "repeated_hidden_state: torch.Size([1, 4, 100, 4096])\n",
      "hidden_state: torch.Size([1, 100, 4096])\n",
      "repeated_hidden_state: torch.Size([1, 4, 100, 4096])\n",
      "hidden_state: torch.Size([1, 100, 4096])\n",
      "repeated_hidden_state: torch.Size([1, 4, 100, 4096])\n",
      "context_hidden: torch.Size([3, 4, 100, 4096])\n",
      "torch.Size([4, 13])\n",
      "torch.Size([4, 13])\n",
      "norm_context_hidden: torch.Size([4, 100, 4096])\n",
      "norm_next_hidden: torch.Size([4, 1, 4096])\n",
      "torch.Size([4, 100])\n",
      "torch.Size([4, 13])\n",
      "torch.Size([4, 13])\n",
      "norm_context_hidden: torch.Size([4, 100, 4096])\n",
      "norm_next_hidden: torch.Size([4, 1, 4096])\n",
      "torch.Size([4, 100])\n",
      "torch.Size([4, 13])\n",
      "torch.Size([4, 13])\n",
      "norm_context_hidden: torch.Size([4, 100, 4096])\n",
      "norm_next_hidden: torch.Size([4, 1, 4096])\n",
      "torch.Size([4, 100])\n",
      "torch.Size([4, 13])\n",
      "hidden_state: torch.Size([1, 100, 4096])\n",
      "repeated_hidden_state: torch.Size([1, 4, 100, 4096])\n",
      "hidden_state: torch.Size([1, 100, 4096])\n",
      "repeated_hidden_state: torch.Size([1, 4, 100, 4096])\n",
      "hidden_state: torch.Size([1, 100, 4096])\n",
      "repeated_hidden_state: torch.Size([1, 4, 100, 4096])\n",
      "context_hidden: torch.Size([3, 4, 100, 4096])\n",
      "torch.Size([4, 13])\n",
      "torch.Size([4, 13])\n",
      "norm_context_hidden: torch.Size([4, 100, 4096])\n",
      "norm_next_hidden: torch.Size([4, 1, 4096])\n",
      "torch.Size([4, 100])\n",
      "torch.Size([4, 13])\n",
      "torch.Size([4, 13])\n",
      "norm_context_hidden: torch.Size([4, 100, 4096])\n",
      "norm_next_hidden: torch.Size([4, 1, 4096])\n",
      "torch.Size([4, 100])\n",
      "torch.Size([4, 13])\n",
      "torch.Size([4, 13])\n",
      "norm_context_hidden: torch.Size([4, 100, 4096])\n",
      "norm_next_hidden: torch.Size([4, 1, 4096])\n",
      "torch.Size([4, 100])\n",
      "torch.Size([4, 13])\n",
      "hidden_state: torch.Size([1, 100, 4096])\n",
      "repeated_hidden_state: torch.Size([1, 4, 100, 4096])\n",
      "hidden_state: torch.Size([1, 100, 4096])\n",
      "repeated_hidden_state: torch.Size([1, 4, 100, 4096])\n",
      "hidden_state: torch.Size([1, 100, 4096])\n",
      "repeated_hidden_state: torch.Size([1, 4, 100, 4096])\n",
      "context_hidden: torch.Size([3, 4, 100, 4096])\n",
      "torch.Size([4, 13])\n",
      "torch.Size([4, 13])\n",
      "norm_context_hidden: torch.Size([4, 100, 4096])\n",
      "norm_next_hidden: torch.Size([4, 1, 4096])\n",
      "torch.Size([4, 100])\n",
      "torch.Size([4, 13])\n",
      "torch.Size([4, 13])\n",
      "norm_context_hidden: torch.Size([4, 100, 4096])\n",
      "norm_next_hidden: torch.Size([4, 1, 4096])\n",
      "torch.Size([4, 100])\n",
      "torch.Size([4, 13])\n",
      "torch.Size([4, 13])\n",
      "norm_context_hidden: torch.Size([4, 100, 4096])\n",
      "norm_next_hidden: torch.Size([4, 1, 4096])\n",
      "torch.Size([4, 100])\n",
      "torch.Size([4, 13])\n",
      "hidden_state: torch.Size([1, 100, 4096])\n",
      "repeated_hidden_state: torch.Size([1, 4, 100, 4096])\n",
      "hidden_state: torch.Size([1, 100, 4096])\n",
      "repeated_hidden_state: torch.Size([1, 4, 100, 4096])\n",
      "hidden_state: torch.Size([1, 100, 4096])\n",
      "repeated_hidden_state: torch.Size([1, 4, 100, 4096])\n",
      "context_hidden: torch.Size([3, 4, 100, 4096])\n",
      "torch.Size([4, 13])\n",
      "torch.Size([4, 13])\n",
      "norm_context_hidden: torch.Size([4, 100, 4096])\n",
      "norm_next_hidden: torch.Size([4, 1, 4096])\n",
      "torch.Size([4, 100])\n",
      "torch.Size([4, 13])\n",
      "torch.Size([4, 13])\n",
      "norm_context_hidden: torch.Size([4, 100, 4096])\n",
      "norm_next_hidden: torch.Size([4, 1, 4096])\n",
      "torch.Size([4, 100])\n",
      "torch.Size([4, 13])\n",
      "torch.Size([4, 13])\n",
      "norm_context_hidden: torch.Size([4, 100, 4096])\n",
      "norm_next_hidden: torch.Size([4, 1, 4096])\n",
      "torch.Size([4, 100])\n",
      "torch.Size([4, 13])\n",
      "hidden_state: torch.Size([1, 100, 4096])\n",
      "repeated_hidden_state: torch.Size([1, 4, 100, 4096])\n",
      "hidden_state: torch.Size([1, 100, 4096])\n",
      "repeated_hidden_state: torch.Size([1, 4, 100, 4096])\n",
      "hidden_state: torch.Size([1, 100, 4096])\n",
      "repeated_hidden_state: torch.Size([1, 4, 100, 4096])\n",
      "context_hidden: torch.Size([3, 4, 100, 4096])\n",
      "torch.Size([4, 13])\n",
      "torch.Size([4, 13])\n",
      "norm_context_hidden: torch.Size([4, 100, 4096])\n",
      "norm_next_hidden: torch.Size([4, 1, 4096])\n",
      "torch.Size([4, 100])\n",
      "torch.Size([4, 13])\n",
      "torch.Size([4, 13])\n",
      "norm_context_hidden: torch.Size([4, 100, 4096])\n",
      "norm_next_hidden: torch.Size([4, 1, 4096])\n",
      "torch.Size([4, 100])\n",
      "torch.Size([4, 13])\n",
      "torch.Size([4, 13])\n",
      "norm_context_hidden: torch.Size([4, 100, 4096])\n",
      "norm_next_hidden: torch.Size([4, 1, 4096])\n",
      "torch.Size([4, 100])\n",
      "torch.Size([4, 13])\n",
      "hidden_state: torch.Size([1, 100, 4096])\n",
      "repeated_hidden_state: torch.Size([1, 4, 100, 4096])\n",
      "hidden_state: torch.Size([1, 100, 4096])\n",
      "repeated_hidden_state: torch.Size([1, 4, 100, 4096])\n",
      "hidden_state: torch.Size([1, 100, 4096])\n",
      "repeated_hidden_state: torch.Size([1, 4, 100, 4096])\n",
      "context_hidden: torch.Size([3, 4, 100, 4096])\n",
      "torch.Size([4, 13])\n",
      "torch.Size([4, 13])\n",
      "norm_context_hidden: torch.Size([4, 100, 4096])\n",
      "norm_next_hidden: torch.Size([4, 1, 4096])\n",
      "torch.Size([4, 100])\n",
      "torch.Size([4, 13])\n",
      "torch.Size([4, 13])\n",
      "norm_context_hidden: torch.Size([4, 100, 4096])\n",
      "norm_next_hidden: torch.Size([4, 1, 4096])\n",
      "torch.Size([4, 100])\n",
      "torch.Size([4, 13])\n",
      "torch.Size([4, 13])\n",
      "norm_context_hidden: torch.Size([4, 100, 4096])\n",
      "norm_next_hidden: torch.Size([4, 1, 4096])\n",
      "torch.Size([4, 100])\n",
      "torch.Size([4, 13])\n",
      "existing_hidden_states: torch.Size([3, 1, 100, 4096])\n",
      "last_hidden_states.unsqueeze(0): torch.Size([1, 1, 20, 4096])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Expected size 100 but got size 20 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 2\u001b[0m     generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpenalty_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/root/AnDsim/outputs/test_hidden_states\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# use_token=True,\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m reply \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(generated_ids[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(reply)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AnDsim/transformers/src/transformers/generation/utils.py:2220\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2214\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_stateful:\n\u001b[1;32m   2215\u001b[0m         \u001b[38;5;66;03m# Just like assisted generation, we need to be able to rollback to a previous state (see comment above)\u001b[39;00m\n\u001b[1;32m   2216\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2217\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultidim search is not supported with stateful models, such as \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2218\u001b[0m         )\n\u001b[0;32m-> 2220\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_multidim_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2221\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2227\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2228\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2230\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mSAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH):\n\u001b[1;32m   2231\u001b[0m     \u001b[38;5;66;03m# 11. expand input_ids with `num_return_sequences` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2233\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2234\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2235\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2236\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2237\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AnDsim/transformers/src/transformers/generation/utils.py:3891\u001b[0m, in \u001b[0;36mGenerationMixin._multidim_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3889\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexisting_hidden_states: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexisting_hidden_states\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3890\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast_hidden_states.unsqueeze(0): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlast_hidden_states\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 3891\u001b[0m     updated_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexisting_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_hidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3892\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[1;32m   3893\u001b[0m     updated_hidden_states \u001b[38;5;241m=\u001b[39m last_hidden_states\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Expected size 100 but got size 20 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        penalty_alpha=0.3,\n",
    "        top_k=4,\n",
    "        max_length=20,\n",
    "        hidden_states_file=\"/root/AnDsim/outputs/test_hidden_states\",\n",
    "        # use_token=True,\n",
    "    )\n",
    "\n",
    "reply = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
